{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2decb7f4-baca-4bf7-86c9-aee9b74642e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/ajkayode@outlook.com/ehr-project/ehr-project-bundle/envsetup/Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f25f26b-0f7e-4b60-b538-573003c08ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from typing import Iterator\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def pdf_parser(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    import pymupdf\n",
    "    import pymupdf4llm\n",
    "    \n",
    "    def get_md_from_pdf(b_content):\n",
    "        pdf_doc = pymupdf.Document(stream=b_content, filetype=\"pdf\")\n",
    "        md_text = pymupdf4llm.to_markdown(pdf_doc)\n",
    "        return md_text\n",
    "    \n",
    "    for x in batch_iter:\n",
    "        yield x.apply(get_md_from_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "592ae007-d485-4b39-b433-9acade018aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class JDSilverLoad:\n",
    "    def __init__(self):\n",
    "        spark.conf.set(\n",
    "            \"spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled\", \"true\"\n",
    "        )\n",
    "\n",
    "    def get_start_time(self):\n",
    "        start_time = (\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                select execution_time as start_time \n",
    "            from {ats_configs.jobs_metadate_table_name} \n",
    "            where job_name = '{ats_configs.jd_silver_job_name}'\n",
    "            order by execution_time desc\n",
    "            \"\"\"\n",
    "                ).first()\n",
    "                .asDict()[\"start_time\"]\n",
    "                .strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "\n",
    "        return start_time\n",
    "    \n",
    "    def update_metadata(self, end_time, load_date):\n",
    "        print(f\"Updating metadata for {ats_configs.jd_silver_job_name}\")\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "            insert into {ats_configs.jobs_metadate_table_name}\n",
    "            values('{ats_configs.jd_silver_job_name}', \n",
    "            '{load_date}', \n",
    "            '{end_time}', \n",
    "            'Job Execution Completed.' )\"\"\")\n",
    "        print(f\"Updated metadata for {ats_configs.jd_silver_job_name}\")\n",
    "\n",
    "    def get_load_date(self):\n",
    "        load_date = (\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                select date_add(last_load_date, 1) as load_date \n",
    "                from {ats_configs.jobs_metadate_table_name} \n",
    "                where job_name = '{ats_configs.jd_silver_job_name}'\n",
    "                order by last_load_date desc\n",
    "                \"\"\").first()\n",
    "                .asDict()[\"load_date\"]\n",
    "                .strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "\n",
    "        return load_date\n",
    "\n",
    "    def get_end_time(self):\n",
    "        end_time = (\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                select current_timestamp() as end_time\n",
    "                \"\"\").first()\n",
    "                .asDict()[\"end_time\"]\n",
    "                .strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "        return end_time\n",
    "    \n",
    "    def get_prompt(self):\n",
    "        from datetime import datetime\n",
    "        current_year = datetime.now().year\n",
    "        prompt = f\"\"\"\n",
    "        Extract the following fields from the provided job description and return them as a single JSON object:\n",
    "        \n",
    "        job_title\n",
    "        \n",
    "        required_skills (list)\n",
    "        \n",
    "        nice_to_have_skills (list)\n",
    "        \n",
    "        education (string; required degree(s) or field(s))\n",
    "        \n",
    "        experience_level (string; e.g., 2+ years, Entry Level, etc.)\n",
    "        \n",
    "        responsibilities (list)\n",
    "        \n",
    "        location (string; if available)\n",
    "        \n",
    "        Ignore sections about company overview, benefits, perks, application process, or any information not directly relevant to the candidates qualifications or job requirements.\n",
    "        \n",
    "        For skills, separate required skills (explicitly stated as required, must have, or essential) and nice-to-have skills (those listed as preferred, bonus, or optional)\n",
    "        \n",
    "        If a field is missing, set its value to null (for strings) or an empty array (for lists)\n",
    "        Job Description:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def extract_JD(self):\n",
    "        from pyspark.sql.functions import expr\n",
    "        #set all required time variables\n",
    "        load_date = self.get_load_date()\n",
    "        start_time= self.get_start_time()\n",
    "        end_time= self.get_end_time()\n",
    "\n",
    "        #read change data from bronze layer table\n",
    "        bronze_jd_df = (\n",
    "            spark.read\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingTimestamp\", start_time)\n",
    "            .option(\"endingTimestamp\", end_time)\n",
    "            .table(ats_configs.jd_bronze_table_name)\n",
    "        )\n",
    "\n",
    "        #parse pdf binary to md text and write to silver staging table\n",
    "        parsed_jd_df = (\n",
    "            bronze_jd_df.withColumn(\"text_content\", pdf_parser(\"content\"))\n",
    "            .selectExpr(\n",
    "                \"path as source\",\n",
    "                \"text_content\"\n",
    "            )\n",
    "            .write.mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{ats_configs.jd_silver_table_name}_stg\")\n",
    "            )\n",
    "\n",
    "        #prepare prompt to get json content from md text\n",
    "        prompt = self.get_prompt()\n",
    "\n",
    "        #extract key information using llm model to get json object from the md text\n",
    "        jd_extract_df = (spark.read.table(f\"{ats_configs.jd_silver_table_name}_stg\")\n",
    "                              .withColumn('json_context', \n",
    "                                          expr(f\"\"\"ai_query(endpoint => '{ats_configs.chat_model_endpoint_name}',\n",
    "                                                    request => CONCAT('{prompt}', text_content)) \n",
    "                                                               \"\"\"),\n",
    "                                          )\n",
    "        )\n",
    "\n",
    "        #save the silver table result\n",
    "        jd_extract_df.write.mode(\"append\").saveAsTable(ats_configs.jd_silver_table_name)\n",
    "\n",
    "        #update metadata\n",
    "        self.update_metadata(end_time = end_time, load_date = load_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b3872bf-986d-4032-8375-a5f734082763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "JDSilver =JDSilverLoad()\n",
    "JDSilver.extract_JD()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "JD_Bronze_to_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
