{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717a2e02-095b-47d6-96bd-8db7c11e7a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7dabf2f-773f-4d46-a279-f1b1563773c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./model_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7976e41-9b12-4a4b-a0d5-cddb899585cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./vectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eab804d-140f-48d1-b783-7aa738ac8193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Setup:\n",
    "  def __init__(self):\n",
    "      from mlflow.deployments import get_deploy_client\n",
    "      from databricks.vector_search.client import VectorSearchClient\n",
    "      self.vs_client = VectorSearchClient(disable_notice=True)\n",
    "      self.dp_client = get_deploy_client('databricks')\n",
    "  \n",
    "  def setup_landing_zone(self):\n",
    "      print(f\"Setting up landing zone directories....\", end=\"\")\n",
    "      profile_landing_zone = f\"/Volumes/{ats_configs.catalog}/{ats_configs.db}/{ats_configs.profile_landing_zone}\"\n",
    "      dbutils.fs.mkdirs(profile_landing_zone)\n",
    "\n",
    "      js_landing_zone = f\"/Volumes/{ats_configs.catalog}/{ats_configs.db}/{ats_configs.jd_landing_zone}\"\n",
    "      dbutils.fs.mkdirs(js_landing_zone)\n",
    "\n",
    "      print('done')\n",
    "      \n",
    "  def setup_tables(self):\n",
    "      print(f\"Setting up tables....\", end=\"\")\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {ats_configs.profile_bronze_table_name}(\n",
    "          id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "          path string,\n",
    "          modificationTime timestamp,\n",
    "          length BIGINT,\n",
    "          content binary\n",
    "      ) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "                \"\"\")\n",
    "      print(f\"{ats_configs.profile_bronze_table_name} created...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {ats_configs.profile_silver_table_name}_stg(\n",
    "          source string,\n",
    "          text_content string\n",
    "      )\n",
    "          \"\"\")\n",
    "      \n",
    "      print(f\"{ats_configs.profile_silver_table_name}_stg created...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {ats_configs.profile_silver_table_name}(\n",
    "                id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "                source string,\n",
    "                text_content string,\n",
    "                json_context string\n",
    "                ) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "                \"\"\")\n",
    "      print(f\"{ats_configs.profile_silver_table_name} created...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {ats_configs.jd_bronze_table_name}(\n",
    "          id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "          path string,\n",
    "          modificationTime timestamp,\n",
    "          length BIGINT,\n",
    "          content binary\n",
    "      ) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "                \"\"\")\n",
    "      print(f\"{ats_configs.jd_bronze_table_name} created...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {ats_configs.jd_silver_table_name}_stg(\n",
    "          source string,\n",
    "          text_content string\n",
    "      )\n",
    "          \"\"\")\n",
    "      \n",
    "      print(f\"{ats_configs.jd_silver_table_name}_stg created...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {ats_configs.jd_silver_table_name}(\n",
    "                id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "                source string,\n",
    "                text_content string,\n",
    "                json_context string\n",
    "                ) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "                \"\"\")\n",
    "      print(f\"{ats_configs.jd_silver_table_name} created...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {ats_configs.jd_profile_table_name}_stg(\n",
    "          jd_id bigint,\n",
    "          profile_id bigint\n",
    "      )\n",
    "          \"\"\")\n",
    "      \n",
    "      print(f\"{ats_configs.jd_profile_table_name}_stg created...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {ats_configs.jd_profile_table_name}(\n",
    "          jd_id bigint,\n",
    "          jd_source string,\n",
    "          jd_extract string,\n",
    "          profile_id bigint,\n",
    "          profile_source string,\n",
    "          profile_extract string,\n",
    "          summary string,\n",
    "          generated_date timestamp\n",
    "      ) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "          \"\"\")\n",
    "      \n",
    "      print(f\"{ats_configs.jd_profile_table_name} created...\", end=\"\")\n",
    "\n",
    "  def setup_jobs_metadata(self):\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {ats_configs.jobs_metadate_table_name}(\n",
    "          job_name string,\n",
    "          last_load_date date,\n",
    "          execution_time timestamp,\n",
    "          description string\n",
    "      )\n",
    "          \"\"\")\n",
    "      \n",
    "      print(f\"{ats_configs.jobs_metadate_table_name} created...\", end=\"\")\n",
    "\n",
    "      print(f\"loading initial config insert for{ats_configs.jobs_metadate_table_name}...\", end=\"\")\n",
    "\n",
    "      spark.sql(f\"\"\"\n",
    "                INSERT INTO {ats_configs.jobs_metadate_table_name}\n",
    "                VALUES ('{ats_configs.profile_ingestion_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}'),\n",
    "                ('{ats_configs.jd_ingestion_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}'),\n",
    "                ('{ats_configs.profile_bronze_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}'),\n",
    "                ('{ats_configs.profile_silver_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}'),\n",
    "                ('{ats_configs.jd_bronze_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}'),\n",
    "                ('{ats_configs.jd_silver_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}'),\n",
    "                ('{ats_configs.jd_profile_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}'),\n",
    "                ('{ats_configs.index_sync_job_name}',try_cast(current_timestamp() as date),current_timestamp(),'{ats_configs.jobs_Initial_description}')\n",
    "                \"\"\")\n",
    "      \n",
    "      print('initial config insert for{ats_configs.jobs_metadate_table_name} loaded...', end=\"\")\n",
    "\n",
    "  def setup_endpoint(self):\n",
    "      endpoint = Endpoints()\n",
    "      endpoint.create(name=ats_configs.chat_model_endpoint_name, model=ats_configs.chat_model_name)\n",
    "      print(f\"Chat endpoint {ats_configs.chat_model_endpoint_name} created\")\n",
    "\n",
    "      endpoint.create(name=ats_configs.embedding_model_endpoint_name, model=ats_configs.embedding_model_name, task ='embeddings')\n",
    "      print(f\"Embeding endpoint {ats_configs.embedding_model_endpoint_name} created\")\n",
    "\n",
    "  def setup_vector_index(self):\n",
    "      vs = VectorSearch()\n",
    "      vs.create_vector_endpoint(endpoint_name=ats_configs.vector_store_endpoint_name)\n",
    "      print(f\"Vector store endpoint {ats_configs.vector_store_endpoint_name} created\")\n",
    "\n",
    "      vs.create_vector_index(\n",
    "          index_name=ats_configs.vector_index_name, \n",
    "          source_table_name=f'{ats_configs.catalog}.{ats_configs.db}.{ats_configs.profile_silver_table_name}',\n",
    "          primary_key=\"id\",\n",
    "          embedding_source_column = 'json_context',\n",
    "          embedding_model_endpoint_name=ats_configs.embedding_model_endpoint_name,\n",
    "          vector_endpoint_name=ats_configs.vector_store_endpoint_name\n",
    "      )\n",
    "\n",
    "  def run(self):\n",
    "      if not spark.catalog.tableExists(f\"{ats_configs.catalog}.{ats_configs.db}.{ats_configs.jobs_metadate_table_name}\"):\n",
    "          self.setup_jobs_metadata()\n",
    "      elif(spark.sql(f\"SELECT COUNT(*) as cnt FROM {ats_configs.jobs_metadate_table_name}\").collect()[0]['cnt'] < 8):\n",
    "          self.setup_jobs_metadata()\n",
    "      self.setup_landing_zone()\n",
    "      self.setup_tables()\n",
    "      self.setup_endpoint()\n",
    "      self.setup_vector_index()\n",
    "\n",
    "  def assert_table(self, table_name):\n",
    "      assert spark.catalog.tableExists(f\"{ats_configs.catalog}.{ats_configs.db}.{table_name}\"), f\"Table {table_name} does not exist\"\n",
    "      print(f\"Table {table_name} exists\")\n",
    "  def assert_count(self, table_name, expected_count):\n",
    "      actual_count = spark.read.table(f\"{ats_configs.catalog}.{ats_configs.db}.{table_name}\").count()\n",
    "      assert actual_count == expected_count, f\"Table {table_name} has {actual_count} rows, expected {expected_count}\"\n",
    "      print(f\"Table {table_name} has {actual_count} rows\")\n",
    "  \n",
    "  def assert_endpoint(self, endpoint_name):\n",
    "      endpoint_list = self.dp_client.list_endpoints()\n",
    "      endpoint_names = [ep[\"name\"] for ep in endpoint_list]\n",
    "      assert endpoint_name in endpoint_names, f\"Endpoint {endpoint_name} does not exist\"\n",
    "      print(f\"Endpoint {endpoint_name} exists\")\n",
    "\n",
    "  def assert_vector_index(self, endpoint_name, index_name):\n",
    "      ep_list = self.vs_client.list_endpoints()\n",
    "      endpoint_names = [ep['name'] for ep in ep_list['endpoints']]\n",
    "      print(endpoint_names)\n",
    "      assert endpoint_name in endpoint_names, f\"Vector endpoint: {endpoint_name} does not exist\"\n",
    "      print(f\"Vector endpoint: {endpoint_name} exists\")\n",
    "\n",
    "      index_list = self.vs_client.list_indexes(endpoint_name)\n",
    "      index_names = [index[\"name\"] for index in index_list[\"vector_indexes\"]]\n",
    "      print(index_names)\n",
    "      assert index_name in index_names, f\"Vector index {index_name} does not exist\"\n",
    "      print(f\"Vector index {index_name} exists\")\n",
    "  def assert_dir(self, dir_name):\n",
    "      dirs = dbutils.fs.ls(f\"/Volumes/{ats_configs.catalog}/{ats_configs.db}/{ats_configs.landing_volume}\")\n",
    "      dir_list = [dir.path for dir in dirs]\n",
    "      dir_path = f\"dbfs:/Volumes/{ats_configs.catalog}/{ats_configs.db}/{ats_configs.landing_volume}/{dir_name}/\"\n",
    "      assert dir_path in dir_list, f\"Directory {dir_path} does not exist\"\n",
    "      print(f\"Directory {dir_path} exists\")\n",
    "\n",
    "  def validate(self):\n",
    "      import time\n",
    "      start_time = int(time.time())\n",
    "      print(f\"Starting validation at {start_time}\")\n",
    "\n",
    "      assert spark.catalog.databaseExists(f\"{ats_configs.db}\")\n",
    "      self.assert_table(ats_configs.profile_bronze_table_name)\n",
    "      self.assert_table(ats_configs.profile_silver_table_name)\n",
    "      self.assert_table(ats_configs.jd_profile_table_name)\n",
    "      self.assert_table(ats_configs.jd_bronze_table_name)\n",
    "      self.assert_table(ats_configs.jd_silver_table_name)\n",
    "      self.assert_table(ats_configs.jobs_metadate_table_name)\n",
    "\n",
    "      self.assert_dir(ats_configs.profile_source)\n",
    "      self.assert_dir(ats_configs.jd_source)\n",
    "\n",
    "      self.assert_count(ats_configs.jobs_metadate_table_name, 8)\n",
    "\n",
    "      self.assert_endpoint(ats_configs.chat_model_endpoint_name)\n",
    "      self.assert_endpoint(ats_configs.embedding_model_endpoint_name)\n",
    "      self.assert_vector_index(ats_configs.vector_store_endpoint_name, ats_configs.vector_index_name)\n",
    "\n",
    "      print(f\"Completed validation in {int(time.time()) - start_time} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84dc9a26-0ea7-42fc-b076-fbf5c0ea00dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "setup = Setup()\n",
    "#setup.run()\n",
    "setup.validate()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
