{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c835480d-b941-43b2-8dcd-f52bb0ba42a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/ajkayode@outlook.com/ehr-project/ehr-project-bundle/envsetup/Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b80c98-6a41-419c-a171-839a3dcf5e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from typing import Iterator\n",
    "\n",
    "#broadcast_conf = spark.sparkContext.broadcast(f\"{ats_configs.catalog}.{ats_configs.db}\")\n",
    "broadcast_index = spark.sparkContext.broadcast(f\"{ats_configs.vector_index_name}\")\n",
    "\n",
    "@pandas_udf(\"array<bigint>\")\n",
    "def profile_search(content: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "    vs = VectorSearchClient()\n",
    "    #index = vs.get_index(index_name = ats_configs.vector_index_name)\n",
    "    index = vs.get_index(index_name = broadcast_index.value)\n",
    "\n",
    "    def get_profiles(content):\n",
    "        ids = index.similarity_search(query_text=content, columns=[\"id\"], num_results=2)\n",
    "        return [int(data[0]) for data in ids[\"result\"][\"data_array\"]]\n",
    "\n",
    "    for series in content:\n",
    "        yield series.apply(get_profiles)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db5564a-e8b9-4c24-8217-ef63be399b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ProfileJDMatching:\n",
    "    def __init__(self):\n",
    "        spark.conf.set(\n",
    "            \"spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled\", \"true\"\n",
    "        )\n",
    "\n",
    "    def get_start_time(self):\n",
    "        start_time = (\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                select execution_time as start_time \n",
    "            from {ats_configs.jobs_metadate_table_name} \n",
    "            where job_name = '{ats_configs.jd_profile_job_name}'\n",
    "            order by execution_time desc\n",
    "            \"\"\"\n",
    "                ).first()\n",
    "                .asDict()[\"start_time\"]\n",
    "                .strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "\n",
    "        return start_time\n",
    "\n",
    "    def get_load_date(self):\n",
    "        load_date = (\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                select date_add(last_load_date, 1) as load_date \n",
    "                from {ats_configs.jobs_metadate_table_name} \n",
    "                where job_name = '{ats_configs.jd_profile_job_name}'\n",
    "                order by last_load_date desc\n",
    "                \"\"\").first()\n",
    "                .asDict()[\"load_date\"]\n",
    "                .strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "\n",
    "        return load_date\n",
    "\n",
    "    def get_end_time(self):\n",
    "        end_time = (\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                select current_timestamp() as end_time\n",
    "                \"\"\").first()\n",
    "                .asDict()[\"end_time\"]\n",
    "                .strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "        return end_time\n",
    "\n",
    "    def update_metadata(self, end_time, load_date):\n",
    "        print(f\"Updating metadata for {ats_configs.jd_profile_job_name}\")\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "            insert into {ats_configs.jobs_metadate_table_name}\n",
    "            values('{ats_configs.jd_profile_job_name}', \n",
    "            '{load_date}', \n",
    "            '{end_time}', \n",
    "            'Job Execution Completed.' )\"\"\")\n",
    "        print(f\"Updated metadata for {ats_configs.jd_profile_job_name}\")\n",
    "\n",
    "    def get_prompt(self):\n",
    "        prompt = f\"\"\"\n",
    "        For the candidate resume below, compare it to the provided job description.\n",
    "        Provide your output as a JSON object with:\n",
    "        name (from the resume)\n",
    "\n",
    "        email (from the resume)\n",
    "\n",
    "        phone (from the resume)\n",
    "\n",
    "        fit_score (number from 0 to 10, where 10 is a perfect fit)\n",
    "\n",
    "        matched_skills (list of skills present in both resume and JD)\n",
    "\n",
    "        missing_skills (list of required JD skills not found in the resume)\n",
    "\n",
    "        evaluation (a short, objective summary of the candidateâ€™s fit, mentioning strengths and gaps)\n",
    "\n",
    "        When comparing skills:\n",
    "        - Focus the fit score and evaluation primarily on technical and role-specific skills.\n",
    "        - Do not penalize a candidate for missing foundational technical skills (such as object-oriented programming, data structures, algorithms) or soft skills (such as communication skills, analytical skills, teamwork) if their education, job titles, or work experience clearly imply these skills.\n",
    "        - Infer such skills from relevant job titles, degrees, leadership, or collaborative work\n",
    "        - Only list these as missing if there is clear evidence the candidate lacks them or if their experience is too junior to reasonably assume them.\n",
    "\n",
    "        Return a list of such JSON objects, one per resume.\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def get_jd_profile_matching(self):\n",
    "        from pyspark.sql.functions import expr, explode\n",
    "\n",
    "        # Read change data from the JD silver layer\n",
    "        load_date = self.get_load_date()\n",
    "        start_time= self.get_start_time()\n",
    "        end_time= self.get_end_time()\n",
    "\n",
    "        #read change data from bronze layer table\n",
    "        silver_jd_df = (\n",
    "            spark.read\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingTimestamp\", start_time)\n",
    "            .option(\"endingTimestamp\", end_time)\n",
    "            .table(ats_configs.jd_silver_table_name)\n",
    "        )\n",
    "\n",
    "        # Search for marching profiles for each JD from vector index and Save matching in gold staging table\n",
    "        jd_profile_df = (silver_jd_df.withColumn(\"profile_id\", explode(profile_search(\"json_context\")))\n",
    "                                    .selectExpr('id as jd_id', \n",
    "                                                'profile_id')\n",
    "                                    .write.mode(\"overwrite\")\n",
    "                                    .saveAsTable(f\"{ats_configs.jd_profile_table_name}_stg\")\n",
    "        )\n",
    "\n",
    "        # Read from gold staging and use LLM to analyze each profile with JD and summarize for fitness\n",
    "        prompt = self.get_prompt()\n",
    "\n",
    "        jd_profile_df = spark.read.table(f\"{ats_configs.jd_profile_table_name}_stg\")\n",
    "        profile_df = spark.read.table(ats_configs.profile_silver_table_name)\n",
    "        jd_df = spark.read.table(ats_configs.jd_silver_table_name)\n",
    "\n",
    "        matching_jd_profile_df = (\n",
    "            jd_profile_df.alias(\"stg_df\")\n",
    "            .join(profile_df.alias(\"profile\"), jd_profile_df.profile_id == profile_df.id)\n",
    "            .join(jd_df.alias(\"jd\"), jd_profile_df.jd_id == jd_df.id)\n",
    "            .selectExpr(\"stg_df.jd_id\",\n",
    "                    \"jd.source as jd_source\",\n",
    "                    \"jd.json_context as jd_extract\",\n",
    "                    \"stg_df.profile_id\",\n",
    "                    \"profile.source as profile_source\",\n",
    "                    \"profile.json_context as profile_extract\"\n",
    "        )\n",
    "        )\n",
    "\n",
    "        jd_profile_summary_df = matching_jd_profile_df.withColumn(\"summary\",\n",
    "                                                                  expr(f\"\"\"ai_query(endpoint => '{ats_configs.chat_model_endpoint_name}',\n",
    "                                                                    request => CONCAT('{prompt}', \n",
    "                                                                                        'Resume: ',profile_extract,\n",
    "                                                                                        'Job Description: ', jd_extract)) \n",
    "                                                                        \"\"\"),\n",
    "                                                                  ).withColumn(\"generated_date\", expr(\"current_timestamp()\"))\n",
    "        # Save the result in the Gold layer\n",
    "        jd_profile_summary_df.write.mode(\"append\").saveAsTable(ats_configs.jd_profile_table_name)\n",
    "\n",
    "        # Update job metadata for gold layer execution\n",
    "        self.update_metadata(end_time, load_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c9ff65-df72-49d4-b580-e18fdc97de89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ProfileDJMatch = ProfileJDMatching()\n",
    "ProfileDJMatch.get_jd_profile_matching()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Profile_JD_Matching_gl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
