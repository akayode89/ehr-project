{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a917b7b-6748-42c7-ae10-d2d0ee8b6558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e2c3bfb-7eec-458c-9ae4-b826b32d7901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ProfileIngestion:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make_source_dir(self):\n",
    "        src_dir = (\n",
    "            spark.sql(\n",
    "            f\"\"\"select date_add(last_load_date, 1) as source_dir \n",
    "            from {ats_configs.jobs_metadate_table_name} \n",
    "            where job_name = '{ats_configs.profile_ingestion_job_name}'\n",
    "            order by last_load_date desc\n",
    "            \"\"\").first()\n",
    "                .asDict()[\"source_dir\"]\n",
    "        )\n",
    "        return src_dir\n",
    "    \n",
    "    def cleanup_destination_dir(self, dest_path):\n",
    "        print(f\"Deleting {dest_path}\")\n",
    "        dbutils.fs.rm(dest_path, recurse=True)\n",
    "        print(f\"Deleted {dest_path} if already exists\")\n",
    "\n",
    "        dbutils.fs.mkdirs(dest_path)\n",
    "        print(f\"Created {dest_path}\")\n",
    "    \n",
    "    def update_metadata(self, ingestion_date):\n",
    "        print(f\"Updating metadata for {ats_configs.profile_ingestion_job_name}\")\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "            insert into {ats_configs.jobs_metadate_table_name}\n",
    "            values('{ats_configs.profile_ingestion_job_name}', ingestion_date, current_timestamp(), 'Job Execution Completed.' )\"\"\")\n",
    "        print(f\"Updated metadata for {ats_configs.profile_ingestion_job_name}\")\n",
    "\n",
    "    def ingest_profiles():\n",
    "        print(\"Ingesting profiles\")\n",
    "        import requests        \n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        import collections\n",
    "\n",
    "        download_dir = self.make_source_dir()\n",
    "        dest_path = f\"/Volumes/{ats_configs.catalog}/{ats_configs.db}/{ats_configs.profile_landing_zone}/{download_dir}\"\n",
    "        self.cleanup_destination_dir(dest_path)\n",
    "\n",
    "        api_url = f\"https://api.github.com/repos/{ats_configs.owner}/{ats_configs.repo}/contents/{ats_configs.profile_source}/{download_dir}\"\n",
    "        files = requests.get(api_url).json()\n",
    "        download_urls = [file[\"download_url\"] for file in files]\n",
    "\n",
    "        def download_files(download_url):\n",
    "            filename = download_url.split(\"/\")[-1]\n",
    "            with requests.get(download_url, stream=True) as r:                \n",
    "                with open(f\"{dest_path}/{filename}\", \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)               \n",
    "\n",
    "        print(f\"Downloading files...\", end='')\n",
    "        with ThreadPoolExecutor(max_workers=4) as executors:\n",
    "            collections.deque(executors.map(download_files, download_urls))\n",
    "        print(\"Done\")\n",
    "        return dest_path\n",
    "        \n",
    "    def assert_file_count(self, dir_name, expected_count):\n",
    "        file_count = len([file.path for file in dbutils.fs.ls(dir_name)])\n",
    "        assert file_count == expected_count, f\"Expected {expected_count} files in {dir_name} but found {file_count}\"\n",
    "        print(f\"Found {file_count} files in {dir_name}. Successful match expected count\")\n",
    "        \n",
    "    def validate(self, iter):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        print(\"Validating files\")\n",
    "        for file in iter:\n",
    "        self.assert_file_count('2025-07-01' if iter == 1 else '2025-07-02', 5)\n",
    "        end = time.time()\n",
    "        print(f\"Validated files in {end - start} seconds\")\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "profile_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
